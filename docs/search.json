[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lucas S. Macoris",
    "section": "",
    "text": "Welcome to my personal website!\nI am an Applied Economist/Data Scientist who works as a Media Marketing Mix/Data Science Consultant at Circana. I am also a Ph.D. Candidate in Economics at INSPER (Institute of Education and Research, Sao Paulo). By sharing random thoughts and ideas with a few lines of code, I aim to bridge the gap between what I am passionate about and what the broader community can learn about it. Hopefully, I can make someone passionate about these things, too.\n\nsuppressWarnings(message(readLines('hard-truth.txt')))\n\n\"If the statistics are boring, you've got the wrong numbers.\" -- Edward Tufte"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "I have received my Bachelor’s (in Management) and Master’s Degree (Finance, Economics, and Econometrics) from the University of Sao Paulo (USP, Sao Paulo, Brazil). I am a Ph.d Candidate in Economics at INSPER (Institute of Education and Research, Sao Paulo, Brazil).\nMy academic background is in Corporate Finance/Applied Economics. More specifically, my academic research lies at the intersection between Corporate Finance and Financial Intermediation. I study how market frictions affect firms’ financing and investment decisions, and what are the implications of these imperfections in terms of managerial action, firms’ future outcomes, and market responses.\nDuring this journey, data in its various formats has (and will always be) been part of my day-to-day activities. I started coding when I first discovered DataCamp and enrolled in my first R programming course. At first, I was terrified by the amount of complexity that it brought, and how challenging it was to move from user-interface schemes (like excel) to programming languages. From time to time, being able to grasp on common questions (literally any) that were already answered by a very supportive community (and, of course, learning from it) made this substantial shift much more enjoyable.\nAcquiring such skills provided me flexibility and allowed me to explore creativity to better express ideas and look for answering interesting research/business questions without the hasle of putting hours of work in repetitive tasks. I used R, Python, SQL, LaTex, and all of its related interfaces in literally everything related to my research agenda, as well as during several oriented courses on applied economics and finance, both at the undergraduate and graduate levels, where I worked as a Teaching Assistant during my Ph.D enrollment.\nAll in all, from what I can say, there’s really nothing that could make me happier in my profession – I hope this page explains you why."
  },
  {
    "objectID": "practice.html",
    "href": "practice.html",
    "title": "What I have been reading recently…",
    "section": "",
    "text": "In case you wonder what are good reading tips, here’s a few list of very interesting, technical papers that I have been reading extensively and I recommend to everyone that is interested in applied economics (and, in some cases, quantitative marketing):"
  },
  {
    "objectID": "academic.html",
    "href": "academic.html",
    "title": "My Academic Research",
    "section": "",
    "text": "My research lies at the intersection between Corporate Finance and Financial Intermediation. More specifically, I study how market frictions affect firms’ financing and investment decisions, and what are the implications of these imperfections in terms of managerial action, firms’ future outcomes, and market responses.\nBelow you can find some of my published/work-in-progress academic work:\n\n\nM&A under financing frictions: evidence from credit supply shortfalls\nJoint work with Luis Ricardo Kabbach de Castro (University of Navarra)\n\nStatus: Working Paper. Draft verison can be obtained here.\nPresentations: 2022 European Financial Management Association (EFMA), 2022 Academy of Management (AOM), Insper, Getulio Vargas Foundation (EAESP), University of Navarra, Brazilian Finance Meeting.\n\nAbstract: despite empirical evidence showing that firms’ investments decrease during periods of credit supply shortfalls, little is known about how firms can eventually circumvent the adverse effects of negative credit supply shocks. In this paper, we show that firms can relieve financing frictions during banking crisis periods by selling equity stakes to outside investors, and the specific channel that relieves such frictions is a shift from domestic to cross-border issuance in countries with historically higher issuance volume. To do so, we examine Mergers and Acquisitions (M&A) transactions worldwide between 1990-2019 and the outcomes of targeted firms the deal by exploring cross-sectional variation in the supply of credit induced by banking crises. The results show that firms that have higher levels of expiring debt maturities in the year of the credit shock are more likely to become targets in M&A deals. Moreover, we find strong evidence that target firms invest more and issue more debt after the deal relative to their counterparts. Finally, we show that these firms are shifting from issuing debt in their own countries to issuing debt in countries with historically higher issuance volume. All in all, these results show that M&As can work as leeway to relieve financing frictions in periods when credit supply frictions are more prevalent.\n\n\n\nMinority Acquisitons and Financial Constraints: reaping big benefits from small shareholders\nJoint work with Luis Ricardo Kabbach de Castro (University of Navarra), Dirk Boehe (Un. of Massey) and Aquiles Kalatzis (University of Sao Paulo)\n\nStatus: Published at the Corporate Governance: an International Review - Click here to access\nPresentations: presented at the XVIII Brazilian Finance (2018) Meeting, accepted at AIB Conference (2018).\n\nAbstract: using a panel of approximately 12,000 domestic and cross-border deals, our results show a positive and statistically significant relationship between the target firms’ financial constraints and the occurrence of minority acquisitions. Moreover, compared to a matched sample, there is a statistically significant difference between the growth in investment and leverage of target firms after deal completion, indicating the effectiveness of minority acquisitions in alleviating target’s financial constraints. At the institutional level, we find that the level of financial development and legal protection in a country are also positively related to minority acquisitions.\n\n\n\nHow has COVID-19 affected air quality in Sao Paulo?\nJoint work with Andre Mancha (Insper) and Naercio Aquino Menezes-Filho (Insper)\n\nStatus: Revise and Resubmit (R&R). Draft version can be obtained here.\n\nAbstract: we assess how social distancing norms following the COVID-19 outbreak affected pollutant levels in Sao Paulo, one of the largest metropolitan areas in the world. We estimate a difference-in-differences model, exploiting exogenous variation in lockdown rules across municipalities to evaluate different mechanisms that have driven air quality improvement in the city. Using hourly air pollution levels measured in thirty-three monitoring stations in the state of Sao Paulo, we find an average decrease of 24.4% in air pollution after the first days of the capital’s quarantine announcement, with heterogeneous effects across types of pollutants. We also compare this effect with exogenous cancellations of traffic restriction rules that occurred between 2000-2018. Our results shed light on the discussion of public policies focused on air quality improvement in large metropolitan areas, which can be a helpful guidance for policymakers in the post-pandemic period.\n\n\n\nOther Work-in-Progress\n\nOn the Study of Debt Structure Determinants – draft can be obtained upon request.\nThe Real Effects of Capital Requirements on the Brazilian Healthcare Industy – draft can be obtained upon request."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Random Thoughts and Ideas",
    "section": "",
    "text": "In this session, I aim to organize some thoughts, ideas, and coding routines that I have implemented along the way. Although I tried my best to organize and trace back all references to cite them properly throughout these posts, I owe a lot to the vibrant R community that is extensively widespread across blog posts, StackOverFlow, and other channels.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAutomating a panel creation of Yahoo! Finance tickers using R\n\n\n\nQuarto\n\n\nR\n\n\n\nA set of ETL routines to collect, treat, and analyze stock price information for a bundle of tickers simultaneously.\n\n\n\nLucas S. Macoris\n\n\nMay 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Test Post\n\n\n\nQuarto\n\n\nR\n\n\n\nDescription\n\n\n\nLucas S. Macoris\n\n\nMay 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Test Post\n\n\n\nQuarto\n\n\nR\n\n\n\nDescription\n\n\n\nLucas S. Macoris\n\n\nMay 15, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/firstpost/firstpost.html",
    "href": "posts/firstpost/firstpost.html",
    "title": "A Test Post",
    "section": "",
    "text": "Here’s a quick example of how to display flextable:\n\nlibrary(flextable)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nmtcars%>%flextable()\n\n\nmpgcyldisphpdratwtqsecvsamgearcarb21.06160.01103.902.62016.46014421.06160.01103.902.87517.02014422.84108.0933.852.32018.61114121.46258.01103.083.21519.44103118.78360.01753.153.44017.02003218.16225.01052.763.46020.22103114.38360.02453.213.57015.84003424.44146.7623.693.19020.00104222.84140.8953.923.15022.90104219.26167.61233.923.44018.30104417.86167.61233.923.44018.90104416.48275.81803.074.07017.40003317.38275.81803.073.73017.60003315.28275.81803.073.78018.00003310.48472.02052.935.25017.98003410.48460.02153.005.42417.82003414.78440.02303.235.34517.42003432.4478.7664.082.20019.47114130.4475.7524.931.61518.52114233.9471.1654.221.83519.90114121.54120.1973.702.46520.01103115.58318.01502.763.52016.87003215.28304.01503.153.43517.30003213.38350.02453.733.84015.41003419.28400.01753.083.84517.05003227.3479.0664.081.93518.90114126.04120.3914.432.14016.70015230.4495.11133.771.51316.90115215.88351.02644.223.17014.50015419.76145.01753.622.77015.50015615.08301.03353.543.57014.60015821.44121.01094.112.78018.601142\n\n\n\n\n\nCitationBibTeX citation:@online{s.macoris2023,\n  author = {Lucas S. Macoris},\n  title = {A {Test} {Post}},\n  date = {2023-05-15},\n  url = {https://lsmacoris.github.io/posts/2022-10-24-quarto-blogs/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nLucas S. Macoris. 2023. “A Test Post.” May 15, 2023. https://lsmacoris.github.io/posts/2022-10-24-quarto-blogs/."
  },
  {
    "objectID": "posts/2023-05-19-Ibovespa-Screening/Ibovespa_Screening.html",
    "href": "posts/2023-05-19-Ibovespa-Screening/Ibovespa_Screening.html",
    "title": "Automating a panel creation of Yahoo! Finance tickers using R",
    "section": "",
    "text": "Here’s a quick example of how to display flextable:\n\nlibrary(flextable)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nmtcars%>%flextable()\n\n\nmpgcyldisphpdratwtqsecvsamgearcarb21.06160.01103.902.62016.46014421.06160.01103.902.87517.02014422.84108.0933.852.32018.61114121.46258.01103.083.21519.44103118.78360.01753.153.44017.02003218.16225.01052.763.46020.22103114.38360.02453.213.57015.84003424.44146.7623.693.19020.00104222.84140.8953.923.15022.90104219.26167.61233.923.44018.30104417.86167.61233.923.44018.90104416.48275.81803.074.07017.40003317.38275.81803.073.73017.60003315.28275.81803.073.78018.00003310.48472.02052.935.25017.98003410.48460.02153.005.42417.82003414.78440.02303.235.34517.42003432.4478.7664.082.20019.47114130.4475.7524.931.61518.52114233.9471.1654.221.83519.90114121.54120.1973.702.46520.01103115.58318.01502.763.52016.87003215.28304.01503.153.43517.30003213.38350.02453.733.84015.41003419.28400.01753.083.84517.05003227.3479.0664.081.93518.90114126.04120.3914.432.14016.70015230.4495.11133.771.51316.90115215.88351.02644.223.17014.50015419.76145.01753.622.77015.50015615.08301.03353.543.57014.60015821.44121.01094.112.78018.601142\n\n\n\n\n\nCitationBibTeX citation:@online{s.macoris2023,\n  author = {Lucas S. Macoris},\n  title = {Automating a Panel Creation of {Yahoo!} {Finance} Tickers\n    Using {R}},\n  date = {2023-05-20},\n  url = {https://lsmacoris.github.io/posts/2023-05-20-Ibovespa-Screening},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nLucas S. Macoris. 2023. “Automating a Panel Creation of Yahoo!\nFinance Tickers Using R.” May 20, 2023. https://lsmacoris.github.io/posts/2023-05-20-Ibovespa-Screening."
  },
  {
    "objectID": "posts/2023-05-20-Ibovespa-Screening/Ibovespa_Screening.html",
    "href": "posts/2023-05-20-Ibovespa-Screening/Ibovespa_Screening.html",
    "title": "Automating a panel creation of Yahoo! Finance tickers using R",
    "section": "",
    "text": "Here’s a quick example of how to display flextable:\n\nlibrary(flextable)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nmtcars%>%flextable()\n\n\nmpgcyldisphpdratwtqsecvsamgearcarb21.06160.01103.902.62016.46014421.06160.01103.902.87517.02014422.84108.0933.852.32018.61114121.46258.01103.083.21519.44103118.78360.01753.153.44017.02003218.16225.01052.763.46020.22103114.38360.02453.213.57015.84003424.44146.7623.693.19020.00104222.84140.8953.923.15022.90104219.26167.61233.923.44018.30104417.86167.61233.923.44018.90104416.48275.81803.074.07017.40003317.38275.81803.073.73017.60003315.28275.81803.073.78018.00003310.48472.02052.935.25017.98003410.48460.02153.005.42417.82003414.78440.02303.235.34517.42003432.4478.7664.082.20019.47114130.4475.7524.931.61518.52114233.9471.1654.221.83519.90114121.54120.1973.702.46520.01103115.58318.01502.763.52016.87003215.28304.01503.153.43517.30003213.38350.02453.733.84015.41003419.28400.01753.083.84517.05003227.3479.0664.081.93518.90114126.04120.3914.432.14016.70015230.4495.11133.771.51316.90115215.88351.02644.223.17014.50015419.76145.01753.622.77015.50015615.08301.03353.543.57014.60015821.44121.01094.112.78018.601142\n\n\n\n\n\nCitationBibTeX citation:@online{s.macoris2023,\n  author = {Lucas S. Macoris},\n  title = {Automating a Panel Creation of {Yahoo!} {Finance} Tickers\n    Using {R}},\n  date = {2023-05-20},\n  url = {https://lsmacoris.github.io/posts/2023-05-20-Ibovespa-Screening},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nLucas S. Macoris. 2023. “Automating a Panel Creation of Yahoo!\nFinance Tickers Using R.” May 20, 2023. https://lsmacoris.github.io/posts/2023-05-20-Ibovespa-Screening."
  },
  {
    "objectID": "posts/2023-05-20-Ibovespa-Screening/index.html",
    "href": "posts/2023-05-20-Ibovespa-Screening/index.html",
    "title": "Automating a panel creation of Yahoo! Finance tickers using R",
    "section": "",
    "text": "This notebook uses Yahoo! Finance to collect data and generate trading signals. The aim of this notebook is to provide an automated screening system in which, based on the technical indicators defined by the researcher, one can easily run a screening daily screening process in order to select a bundle of assets for a given trading strategy. This notebook implements this by using Yahoo! Finance! data on a daily basis.\nFor that, we are going to present a simple example using a set of brazilian traded stocks in Bovespa. All the stocks are presented in the auxiliary .csv file, Assets.csv, and can be changed to accomodate any stock available in Yahoo! Finance servers.\nImportant Remark: Yahoo! Finance generally offers data with splits and dividends adjustments and therefore may not be the same as the brokerage information. In this sense, recommendations must also be analyzed through technical indicators presented on the brokerage account. Additionally, one can change the log in order to use any other provider of financial data of the same format, such as AlphaVantage, which also has an API support into the quantmod library."
  },
  {
    "objectID": "posts/2023-05-20-Ibovespa-Screening/index.html#loading-necessary-libraries",
    "href": "posts/2023-05-20-Ibovespa-Screening/index.html#loading-necessary-libraries",
    "title": "Automating a panel creation of Yahoo! Finance tickers using R",
    "section": "Loading Necessary Libraries",
    "text": "Loading Necessary Libraries\nAlthough this is totally up to you, I’ve selected a few packages that make the overall task easier.\n\npackages.to.install=c(\"ggplot2\",\"dplyr\",\"tidyr\",\"PerformanceAnalytics\",\"quantmod\",\"xts\",\"ggpubr\",\"zoo\",\"quantmod\",\"flextable\")\n\nsapply(packages.to.install,library,character.only = TRUE)\n\n#Read a list of assets\nAssets<-read.csv('Assets.csv',sep=';',encoding = 'UTF-8',header = TRUE,stringsAsFactors = FALSE)[,1]\n\n\n#Let's take a look at the structure of the file\nAssets%>%head(5)\n\n[1] \"ABEV3.SA\" \"AZUL4.SA\" \"B3SA3.SA\" \"BBAS3.SA\" \"BBDC3.SA\"\n\n#Getting Data: now, we are going to request for chunks of 5 assets per time in Yahoo Finance server.\n\nadjust_ticker_data <- function(ticker){\n  \n  return(na.locf(Cl(ticker)))\n  \n}\n\nTickers=list()\n\nfor (i in Assets[1:5]){\n  \nData=adjust_ticker_data(getSymbols(i,auto.assign = FALSE,from='2019-01-01',to=Sys.Date()))\n\nBands = do.call(merge,lapply(Data,BBands))\nRSI = do.call(merge,lapply(Data,RSI))\nSMA= do.call(merge,lapply(Data,SMA))\n\nTickers[[i]]=list(Data=Data,\n                  BBands=Bands,\n                  RSI=RSI,\n                  SMA=SMA)\n\n}\n\n\n#Accessing all RSI\n\nA=do.call(merge,lapply(Tickers,'[[',\"SMA\"))\nnames(A)=Assets[1:5]\n\n## Create color scheme using rainbow()\ntsRainbow = rainbow(ncol(as.zoo(A)))\n\nA%>%plot.xts(COL=tsRainbow)\n\n\n\naddLegend(\"topright\", \n              legend.names=Assets[1:5],\n              col=tsRainbow,\n              lty=c(1,1,1,1,1),\n              lwd=c(2,2,2,2,2),\n              ncol=2,\n              bg=\"white\")\n\n\n\n\n\n#Closing Prices and Return Data: use this wrapper to verify whether there is some Asset with less observations.\n#As we are going to use a data frame, we must keep assets with the same number of observartions. However,\n# one can modify this code and simply treat each asset as a component of a list.\n\nFilter=Assets[which(Assets %in% ls())]"
  },
  {
    "objectID": "posts/2023-05-20-Ibovespa-Screening/index.html#loading-necessary-libraries-1",
    "href": "posts/2023-05-20-Ibovespa-Screening/index.html#loading-necessary-libraries-1",
    "title": "Automating a panel creation of Yahoo! Finance tickers using R",
    "section": "Loading Necessary Libraries",
    "text": "Loading Necessary Libraries\nAlthough this is totally up to you, I’ve selected a few packages that make the overall task easier.\n\n\n$ggplot2\n[1] \"ggplot2\"   \"stats\"     \"graphics\"  \"grDevices\" \"utils\"     \"datasets\" \n[7] \"methods\"   \"base\"     \n\n$dplyr\n[1] \"dplyr\"     \"ggplot2\"   \"stats\"     \"graphics\"  \"grDevices\" \"utils\"    \n[7] \"datasets\"  \"methods\"   \"base\"     \n\n$tidyr\n [1] \"tidyr\"     \"dplyr\"     \"ggplot2\"   \"stats\"     \"graphics\"  \"grDevices\"\n [7] \"utils\"     \"datasets\"  \"methods\"   \"base\"     \n\n$PerformanceAnalytics\n [1] \"PerformanceAnalytics\" \"xts\"                  \"zoo\"                 \n [4] \"tidyr\"                \"dplyr\"                \"ggplot2\"             \n [7] \"stats\"                \"graphics\"             \"grDevices\"           \n[10] \"utils\"                \"datasets\"             \"methods\"             \n[13] \"base\"                \n\n$quantmod\n [1] \"quantmod\"             \"TTR\"                  \"PerformanceAnalytics\"\n [4] \"xts\"                  \"zoo\"                  \"tidyr\"               \n [7] \"dplyr\"                \"ggplot2\"              \"stats\"               \n[10] \"graphics\"             \"grDevices\"            \"utils\"               \n[13] \"datasets\"             \"methods\"              \"base\"                \n\n$xts\n [1] \"quantmod\"             \"TTR\"                  \"PerformanceAnalytics\"\n [4] \"xts\"                  \"zoo\"                  \"tidyr\"               \n [7] \"dplyr\"                \"ggplot2\"              \"stats\"               \n[10] \"graphics\"             \"grDevices\"            \"utils\"               \n[13] \"datasets\"             \"methods\"              \"base\"                \n\n$ggpubr\n [1] \"ggpubr\"               \"quantmod\"             \"TTR\"                 \n [4] \"PerformanceAnalytics\" \"xts\"                  \"zoo\"                 \n [7] \"tidyr\"                \"dplyr\"                \"ggplot2\"             \n[10] \"stats\"                \"graphics\"             \"grDevices\"           \n[13] \"utils\"                \"datasets\"             \"methods\"             \n[16] \"base\"                \n\n$zoo\n [1] \"ggpubr\"               \"quantmod\"             \"TTR\"                 \n [4] \"PerformanceAnalytics\" \"xts\"                  \"zoo\"                 \n [7] \"tidyr\"                \"dplyr\"                \"ggplot2\"             \n[10] \"stats\"                \"graphics\"             \"grDevices\"           \n[13] \"utils\"                \"datasets\"             \"methods\"             \n[16] \"base\"                \n\n$alphavantager\n [1] \"alphavantager\"        \"ggpubr\"               \"quantmod\"            \n [4] \"TTR\"                  \"PerformanceAnalytics\" \"xts\"                 \n [7] \"zoo\"                  \"tidyr\"                \"dplyr\"               \n[10] \"ggplot2\"              \"stats\"                \"graphics\"            \n[13] \"grDevices\"            \"utils\"                \"datasets\"            \n[16] \"methods\"              \"base\"                \n\n$quantmod\n [1] \"alphavantager\"        \"ggpubr\"               \"quantmod\"            \n [4] \"TTR\"                  \"PerformanceAnalytics\" \"xts\"                 \n [7] \"zoo\"                  \"tidyr\"                \"dplyr\"               \n[10] \"ggplot2\"              \"stats\"                \"graphics\"            \n[13] \"grDevices\"            \"utils\"                \"datasets\"            \n[16] \"methods\"              \"base\"                \n\n$flextable\n [1] \"flextable\"            \"alphavantager\"        \"ggpubr\"              \n [4] \"quantmod\"             \"TTR\"                  \"PerformanceAnalytics\"\n [7] \"xts\"                  \"zoo\"                  \"tidyr\"               \n[10] \"dplyr\"                \"ggplot2\"              \"stats\"               \n[13] \"graphics\"             \"grDevices\"            \"utils\"               \n[16] \"datasets\"             \"methods\"              \"base\"                \n\n\n [1] \"ABEV3.SA\" \"AZUL4.SA\" \"B3SA3.SA\" \"BBAS3.SA\" \"BBDC3.SA\" \"BBDC4.SA\"\n [7] \"BBSE3.SA\" \"BRAP4.SA\" \"BRDT3.SA\" \"BRFS3.SA\""
  },
  {
    "objectID": "practice.html#actual-role-1",
    "href": "practice.html#actual-role-1",
    "title": "About me",
    "section": "Actual role",
    "text": "Actual role\nIn late 2021, I have joined Circana (former IRI + NPD), the world’s largest data analytics and market research company. Circana provides clients with consumer, shopper, and retail market intelligence and analysis focused on the consumer packaged goods, retail, and healthcare industries. More specifically, I work as a lead consultant for Media Marketing Mix (MMM) projects for CPG Brands. My work entails collecting, organizing, and treating huge amounts of marketing measurement data, both structured and unstructured."
  },
  {
    "objectID": "practice.html#actual-role-2",
    "href": "practice.html#actual-role-2",
    "title": "About me",
    "section": "Actual role",
    "text": "Actual role\nIn late 2021, I have joined Circana (former IRI + NPD), the world’s largest data analytics and market research company. Circana provides clients with consumer, shopper, and retail market intelligence and analysis focused on the consumer packaged goods, retail, and healthcare industries. More specifically, I work as a lead consultant for Media Marketing Mix (MMM) projects for CPG Brands. My work entails collecting, organizing, and treating huge amounts of marketing measurement data, both structured and unstructured."
  },
  {
    "objectID": "practice.html#actual-role",
    "href": "practice.html#actual-role",
    "title": "About me",
    "section": "Actual role",
    "text": "Actual role\n\n\n50% of my marketing budget is wasted. Which half?\n\n\nIn late 2021, I made a shift from academia and joined Circana (former IRI + NPD), the world’s largest data analytics and market research company. Circana provides clients with consumer, shopper, and retail market intelligence and analysis focused on the consumer packaged goods, retail, and healthcare industries.\nMore specifically, I work as a lead consultant for Media Marketing Mix (MMM) projects for CPG Brands. My work entails collecting, organizing, and treating huge amounts (>>100gb!) of marketing measurement data to estimate econometric models that help our clients to understand how their marketing efforts performed, which marketing drivers levered up the business, and what to do to achieve a higher return on adverstisement spend. Our work helps CPG clients to move millions of dollars every year across towards marketing efforts that have shown to be the top performers, and also understand why they performed as such.\nI use a ton of Data Science and Applied Economics concepts to get my way around this process from and end-to-end perspective. I employ automated ETL (Extract, Treat, and Load) routines to ingest unstructured data, apply econometric models in large scale, and use awesome open-source technologies to analyze and automate output reporting in convenient formats."
  },
  {
    "objectID": "practice.html#marketing-mix-modeling",
    "href": "practice.html#marketing-mix-modeling",
    "title": "What I have been reading recently…",
    "section": "Marketing Mix Modeling",
    "text": "Marketing Mix Modeling\n\nChallenges and Opportunities in Media Mix Modeling\nBayesian Methods for Media Mix Modeling with Carryover and Shape Effects\nA Comparison of Approaches to Advertising Measurement: Evidence from Big Field Experiments at Facebook"
  },
  {
    "objectID": "practice.html#methods-in-applied-econometrics",
    "href": "practice.html#methods-in-applied-econometrics",
    "title": "What I have been reading recently…",
    "section": "Methods in Applied Econometrics",
    "text": "Methods in Applied Econometrics\n\nWhat’s trending in difference-in-differences? A synthesis of the recent econometrics literature\nDifference-in-Differences with multiple time periods\nMostly Harmless Econometrics: an Empiricist’s Companion this has been a good companion since always."
  },
  {
    "objectID": "practice.html#methods-in-applied-microeconomics",
    "href": "practice.html#methods-in-applied-microeconomics",
    "title": "What I have been reading recently…",
    "section": "Methods in Applied (Micro)economics",
    "text": "Methods in Applied (Micro)economics\n\nWhat’s trending in difference-in-differences? A synthesis of the recent econometrics literature\nDifference-in-Differences with multiple time periods\nMostly Harmless Econometrics: an Empiricist’s Companion this has been a good companion since always."
  },
  {
    "objectID": "about.html#actual-role",
    "href": "about.html#actual-role",
    "title": "About me",
    "section": "Actual role",
    "text": "Actual role\n\n\n50% of my marketing budget is wasted. Which half?\n\n\nIn late 2021, I made a shift from academia and joined Circana (former IRI + NPD), the world’s largest data analytics and market research company. Circana provides clients with consumer, shopper, and retail market intelligence and analysis focused on the consumer packaged goods, retail, and healthcare industries.\nMore specifically, I work as a lead consultant for Media Marketing Mix (MMM) projects for CPG Brands. My work entails collecting, organizing, and treating huge amounts (>>100gb!) of marketing measurement data to estimate econometric models that help our clients understand how their marketing efforts performed, which marketing drivers levered up their business, and what to do to achieve a higher return on adverstisement spend. My work helps CPG clients to move millions of dollars every year across towards marketing efforts that have shown to be the top performers, and also understand why they performed as such.\nI use a ton of Data Science and Applied Economics concepts to get my way around this process from and end-to-end perspective. I employ automated ETL (Extract, Treat, and Load) routines to ingest unstructured data, apply econometric models in large scale, and use awesome open-source technologies to analyze and automate output reporting in convenient formats."
  }
]